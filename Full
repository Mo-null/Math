#CO DET ADJ INV




import numpy as np
a=np.array([[1,3,4],[4,3,5],[5,7,8]])
print("your matrix is\n",a)
d=np.linalg.det(a)
print("\ndeterminant of given matrix is:-", int(d))
co=np.linalg.inv(a).T* d
print("\ncofactor matrix of the given matrix is\n",co)
inv=np.linalg.inv(a)
print("\ninverse of given matrix is:-\n",inv)
adj=co.transpose()
print("\nadjoint of a matrix is:-\n",adj)




































#COVERT TO ECHELON / RANK


import numpy as np

def echelon_form(A):
    n, m = np.shape(A)
    for i in range(n):
        pivot = A[i][i]
        for j in range(i+1, n):
            factor = A[j][i] / pivot
            for k in range(i, m):
                A[j][k] = A[j][k] - factor * A[i][k]
    return A

def rank(A):
    echelon_matrix = echelon_form(A)
    n, m = np.shape(echelon_matrix)
    rank = n
    for i in range(n):
        zero_row = True
        for j in range(m):
            if echelon_matrix[i][j] != 0:
                zero_row = False
                break
        if zero_row:
            rank = rank - 1
    return rank

A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print("Echelon Form:")
print(echelon_form(A))
print("Rank:", rank(A))



















#GAUSS ELIMINATION


import numpy as np

def gauss_elimination(A, B):
 
    n = len(B)
    augmented_matrix = [A[i] + [B[i]] for i in range(n)]
    
    for i in range(n):
        max_row = max(range(i, n), key=lambda r: abs(augmented_matrix[r][i]))
        augmented_matrix[i], augmented_matrix[max_row] = augmented_matrix[max_row], augmented_matrix[i]

        pivot = augmented_matrix[i][i]
        if pivot == 0:
            raise ValueError("Matrix is singular and cannot be solved.")
        augmented_matrix[i] = [x / pivot for x in augmented_matrix[i]]

        for j in range(i + 1, n):
            factor = augmented_matrix[j][i]
            augmented_matrix[j] = [
                augmented_matrix[j][k] - factor * augmented_matrix[i][k] for k in range(n + 1)
            ]

    X = [0] * n
    for i in range(n - 1, -1, -1):
        X[i] = augmented_matrix[i][-1] - sum(
            augmented_matrix[i][j] * X[j] for j in range(i + 1, n)
        )
    return X

A = [[2, 1, -1], [-3, -1, 2], [-2, 1, 2]]
B = [8, -11, -3]

solution = gauss_elimination(A, B)
print("Solution:", solution)















#GAUSS JORDAN



import numpy as np
A = np.array([[4, 3, 2], [-2, 2, 3], [3, -5, 2]])
B = np.array([25, -10, -4])
X2 = np.linalg.solve(A,B)

print(X2)
import numpy as np

A = np.array([[4, 3, 2],
              [-2, 2, 3],
              [3, -5, 2]], dtype=float)

def gauss_jordan(A):
    n = len(A)
    
    for i in range(n):

        A[i] = A[i] / A[i][i]
        
        for j in range(n):
            if i != j:
                A[j] = A[j] - A[i] * A[j][i]
                
    return A

result = gauss_jordan(A)

solution = result[:, -1]
print("Solution of the system is:")
print(solution)


















#DEPENDENCE/ 


import numpy as np

def check_linear_dependence(vectors):
    matrix = np.array(vectors).T 
    rank = np.linalg.matrix_rank(matrix)
    return rank < len(vectors)

def linear_combination(vectors, coefficients):
    result = np.dot(coefficients, np.array(vectors))
    return result.tolist()

vectors = [[1, 2, 3], [2, 4, 6], [3, 6, 9]] 
coefficients = [1, -2, 1]  

if check_linear_dependence(vectors):
    print("The vectors are linearly dependent.")
else:
    print("The vectors are linearly independent.")

linear_combo = linear_combination(vectors, coefficients)
print("Linear combination result:", linear_combo)



























#DIAGNOZABLE



import numpy as np

def is_diagonalizable(matrix):
    eigenvalues, eigenvectors = np.linalg.eig(matrix)

    for eigenvalue in eigenvalues:
        eigenspace_matrix = matrix - eigenvalue * np.eye(matrix.shape[0])
        if np.linalg.matrix_rank(eigenspace_matrix) < matrix.shape[0]:
            print("The matrix is diagonalizable.")
            return True

    print("The matrix is not diagonalizable.")
    return False

def find_eigenvalues(matrix):
    eigenvalues, _ = np.linalg.eig(matrix)
    return eigenvalues

matrix = np.array([[4, 1], [2, 3]])

is_diagonalizable(matrix)

eigenvalues = find_eigenvalues(matrix)
print("Eigenvalues:", eigenvalues)

























#GARDIENT/ DIVERGENCE/ CURL



import numpy as np

def gradient(f, x, h=1e-4):
    grad = np.zeros_like(x)
    for i in range(x.shape[0]):
        x_plus, x_minus = np.copy(x), np.copy(x)
        x_plus[i] += h
        x_minus[i] -= h
        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)
    return grad

def divergence(f, x, h=1e-4):
    div = 0
    for i in range(x.shape[0]):
        x_plus, x_minus = np.copy(x), np.copy(x)
        x_plus[i] += h
        x_minus[i] -= h
        div += (f(x_plus)[i] - f(x_minus)[i]) / (2 * h)
    return div

def curl(f, x, h=1e-4):
    c = np.zeros(2)
    for i in range(x.shape[0]):
        x_plus, x_minus = np.copy(x), np.copy(x)
        x_plus[i] += h
        x_minus[i] -= h
        c[i] = (f(x_plus)[(i + 1) % 2] - f(x_minus)[(i + 1) % 2]) / (2 * h)
    c[1], c[0] = -c[0], c[1]
    return c

def f_scalar(x):
    return x[0]**2 + x[1]**2

def f_vector(x):
    return np.array([x[0] + x[1], x[0] - x[1]])

x = np.array([1, 2])
print("Gradient:", gradient(f_scalar, x))
print("Divergence:", divergence(f_vector, x))
print("Curl:", curl(f_vector, x))
